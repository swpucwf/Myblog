# 语音识别算法

## 1. 语音识别简介

### 1.1 **语音识别**

#### 1.1.1 自动语音识别

（Automatic Speech Recognition，ASR)，其目标是将人类的语音转换为文字。

#### 1.1.2 应用

- 离线语音识别（非流式）

指包含语音的音频文件已经存在，需使用语音识别应用对音频的内容进行整体识别。典型应用有音视频会议记录转写、**音频内容分析**及审核、视频字幕生成等。

- 实时在线语音识别（流式）

指包含语音的实时音频流，被连续不断地送入语音识别引擎，过程中获得的识别结果即时返回给调用方。典型应用有手机语音输入法、交互类语音产品（如智能音箱、车载助手）、会场同声字幕生成和翻译、网络直播平台实时监控、电话客服实时质量检测等。

### 1.2 语音识别流程

#### 1.2.1 预处理

预处理(格式转换、压缩编解码、音频数据抽取、声道选择（通常识别引擎只接收单声道数据）、采样率/重采样（常见的识别引擎和模型采样率一般为8kHz、16kHz），FBank**特征提取**。

语音信号被设备接收后 (比如麦克风)，会通过 $A / D$ 转换，将模拟信号转换为数字信号，一般会有采样、量化和编码三个步骤，采样率要遵循奈奎斯特采样定律: $f s>=2 f$ ，比如电话语音的频率一般在 $300 \mathrm{~Hz} \sim 3400 \mathrm{~Hz}$ ，所以采用 $8 \mathrm{kHz}$ 的采样率足矣。

- 将音频文件读取到程序后，它们是一系列离散的采样点，通常采样率是**16k/8k**，即一秒钟采样16000/8000个点，每个采样点表示该时刻声音的振幅。
- 在这个采样率下，一条只有几秒钟的输入音频，其序列长度也会非常长，且每个采样点所包含的语音信息比较少，因此原始音频不适合直接作为模型的输入。无论是传统方法的语音识别技术还是基于神经网络的语音识别技术，都**需要进行语音的预处理。**
- 预处理流程的核心是**快速傅立叶变换**。快速傅立叶变换的作用看似杂乱无章的信号考虑分解为一定振幅、相位、频率的基本正弦(余弦)信号。**傅里叶变换允许我们分解一个信号，以确定不同频率的波的相对强度。**

#### 1.2.2 语音检测和断句

对于离线语音识别应用，断句模块的作用是快速过滤并切分出音频文件中的人声片段，且尽最大可能保证每个片段都为完整的一句话；**对于实时在线语音识别应用**，断句模块则需要能够从一个持续的语音流中，第一时间检测出用户什么时候开始说话（也称为起点检测），以及什么时候说完（也称为尾点检测）。

#### 1.2.3 音频场景分析

除断句外，由于一些应用本身的复杂性，导致原生的音频在被送入识别引擎之前，还需要进一步进行分析和过滤，我们把这类统称为音频场景分析。一般情况**语种识别**也会放在这里。

#### 1.2.4 识别引擎(**语音识别的模型**)

##### 1. 传统语音识别模型

- 经典的语音识别概率模型 ，分为**声学模型**和**语言模型**两部分，现将语音转换为音素，再将音素转换为单词。

- 对于声学模型来说，单词是一个比较大的建模单元，因此声学模型p（Y|w）中的单词序列w会被进一步拆分成一个音素序列。假设Q是单词序列w对应的发音单元序列，这里简化为音素序列，那么声学模型p（Y|w）可以被进一步转写为。一般会用隐马尔可夫模型来进行建模。**音素表，由声学专家定义。**

- 语言模型，使用n-gram模型。

- 传统语音识别 缺点，精度差；优点，速度快可部署在嵌入式设备。

##### 2. 端到端的语音识别模型

2014年左右，谷歌的研究人员发现，在大量数据的支撑下，直接用神经网络可以从输入的音频或音频对应的特征直接预测出与之对应的单词，而**不需要**像我们上面描述的那样，拆分成声学模型和语言模型。简单来说构建一个模型，input ：语音，output：文本即可。

###### **基于Transformer的ASR模型**

- ASR可以被看成是一种序列到序列任务，输入一段声音特征序列，通过模型计算后，输出对应的文字序列信号。在端到端ASR模型中，这种序列到序列的转换**不需要经过中间状态，如音素等，而直接生成输出结果。**
- 基于Transformer的ASR模型，**其输入是提取的FBank或MFCC语音特征。**由于语音特征序列一般比较长，在送入模型之前，通常会进行两层步长为2的卷积操作，将序列变为原来的1/4长。
- **基于Transformer的ASR模型编码器和解码器与原始Transformer没有差别**，在编码器端，是一个多头注意力子层和一个前馈网络子层，它们分别进行残差连接和层标准化（LayerNorm）操作，而在解码器端则会多一个编码器-解码器多头注意力层，用于进行输入和输出之间的交互。
- 缺点:自注意力机制能够对全局的上下文进行建模，不擅长提取细粒度的局部特征模式

######  **基于CNN的ASR模型**

- 基于CNN来捕获局部的特征来进行语音识别，比如 ContextNet。
- 由于受**感受野**范围的限制，CNN只能够在局部的范围内对文本进行建模，相比于RNN或Transformer，缺乏全局的文本信息。ContextNet通过引入“压缩和激发（Squeeze-and-Excitation，SE）”层{Squeeze-and-excitation networks}来获取全局特征信息。
- Squeeze-and-Excitation，SE 模块捕获全局特征的能力， 在图像领域的应用很多。 目标检测，通过引该模块,可以再不损失提取局部特征的能力下，加强对全局特征的提取。即，添加注意力可以加强一定的全局特征提取。但只是通过平均池化和全连接来进行全局信息的交互，这种方法在语音识别领域**仍然无法很好地**获取全局特征。

###### **Conformer**

- 目前业界主流框架，通过一定的组合方式(Transfomer+CNN)应用到ASR任务上。考虑到语音序列建模的特点，Conformer加入了卷积模块，利用CNN的局部建模能力来获取序列的局部特征。
- Conformer结构是在Transformer模型编码器的基础上增加卷积模块, 构成Conformer 模块。
- WeNet 是一款面向工业落地应用的语音识别工具包(框架)，提供了从语音识别模型的训练到部署的一条龙服务，也是目前业界最常用的开源框架。

###### **Paraformer**

- Paraformer 是阿里INTERSPEECH 2022 提出的一个模型，并已经开源。一种具有高识别率与计算效率的单轮非自回归模型 Paraformer。

- 该模型开箱可用，该开源模型的实际识别效果和TEG 的模型效果差不多。**是目前唯一开箱即用的开源预训练模型**， 我认为主要原因是该模型的训练集中包含了工业级别的数据。

#### 1.2.5 工程调度 & 异常处理

工程部署和异常类型处理

### 1.3 基础知识

- 声音（sound)是由物体**振动**产生的声波。是通过**介质**（空气或固体、液体）传播。

- 语音的产生基本原理还是振动。不同位置的震动频率不一样，信号也就不一样，不过信号由**基频**和一些**谐波**构成
- 一个振动产生的波是一个具有一定频率的振幅最大的正弦波叫**基频**。 这些高于基波频率的小波就叫作**谐波**。

![img](https://raw.githubusercontent.com/swpucwf/MyBolgImage/main/images202312040959770.png)

## 2.语音预处理





## 参考笔记

[通俗的讲解语音识别技术 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/635372844)

[ASR中常用的语音特征之FBank和MFCC（原理 + Python实现）_完成fbank、mfcc两种声学特征提取的代码实现-CSDN博客](https://blog.csdn.net/Magical_Bubble/article/details/90295814)