## 张量

本章我们开始介绍Pytorch基础知识，我们从张量说起，建立起对数据的描述，再介绍张量的运算，最后再讲PyTorch 中所有神经网络的核心包 `autograd `，也就是自动微分，了解完这些内容我们就可以较好地理解PyTorch代码了。下面我们开始吧～

**简介**

几何代数中定义的张量是基于向量和矩阵的推广，比如我们可以将标量视为零阶张量，矢量可以视为一阶张量，矩阵就是二阶张量。

- 0维张量/**标量** 标量是一个数字
- 1维张量/**向量**  1维张量称为“向量”。
- 2维张量  2维张量称为**矩阵**
- 3维张量 公用数据存储在张量 时间序列数据 股价 文本数据 彩色图片(**RGB**)

张量是现代机器学习的基础。它的核心是一个数据容器，多数情况下，它包含数字，有时候它也包含字符串，但这种情况比较少。因此可以把它想象成一个数字的水桶。

这里有一些存储在各种类型张量的公用数据集类型：

- **3维=时间序列**
- **4维=图像**
- **5维=视频**

例子：一个图像可以用三个字段表示：

```
(width, height, channel) = 3D
```

但是，在机器学习工作中，我们经常要处理不止一张图片或一篇文档——我们要处理一个集合。我们可能有10,000张郁金香的图片，这意味着，我们将用到4D张量：

```
(sample_size, width, height, channel) = 4D
```

在PyTorch中， torch.Tensor 是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现 Tensor 和NumPy的多维数组非常类似。然而，Tensor 提供GPU计算和自动求梯度等更多功能，这些使 Tensor 这一数据类型更加适合深度学习。

```python
from __future__ import print_function
import torch
```

**创建tensor**

构造一个随机初始化的矩阵：

```python
x = torch.rand(4, 3) 
print(x)
```

构造一个矩阵全为 0，而且数据类型是 long.

```python
x = torch.zeros(4, 3, dtype=torch.long)
print(x)
```

```python
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```

直接使用数据，构造一个张量：

```python
x = torch.tensor([5.5, 3]) 
print(x)
```

```
tensor([5.5000, 3.0000])
```

基于已经存在的 tensor，创建一个 tensor ：

```python
x = x.new_ones(4, 3, dtype=torch.double) # 创建一个新的tensor，返回的tensor默认具有相同的 torch.dtype和torch.device
# 也可以像之前的写法 x = torch.ones(4, 3, dtype=torch.double)
print(x)
x = torch.randn_like(x, dtype=torch.float)
# 重置数据类型
print(x)
# 结果会有一样的size
```

```
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 0.2626, -0.6196,  1.0963],
        [ 1.1366, -0.6543,  0.6040],
        [-0.6623,  0.1115,  0.2433],
        [ 1.1626, -2.3529, -0.9417]])
```

获取它的维度信息：

```python
print(x.size())
print(x.shape)
```

```
torch.Size([4, 3])
```

返回的torch.Size其实就是一个tuple，⽀持所有tuple的操作。

还有一些常见的构造Tensor的函数：

|                  函数 | 功能                                                |
| --------------------: | --------------------------------------------------- |
|      Tensor(**sizes*) | 基础构造函数                                        |
|        tensor(*data*) | 类似于np.array                                      |
|        ones(**sizes*) | 全1                                                 |
|       zeros(**sizes*) | 全0                                                 |
|         eye(**sizes*) | 对角为1，其余为0                                    |
|    arange(*s,e,step*) | 从s到e，步长为step                                  |
| linspace(*s,e,steps*) | 从s到e，均匀分成step份                              |
|  rand/randn(**sizes*) | rand是[0,1)均匀分布；randn是服从N（0，1）的正态分布 |
|    normal(*mean,std*) | 正态分布（均值为mean，标准差是std）                 |
|         randperm(*m*) | 随机排列                                            |

**操作**

一些加法操作：

```python
# 方式1
y = torch.rand(4, 3) 
print(x + y)

# 方式2
print(torch.add(x, y))

# 方式3 提供一个输出 tensor 作为参数
# 这里的 out 不需要和真实的运算结果保持维数一致，但是会有警告提示！
result = torch.empty(5, 3) 
torch.add(x, y, out=result) 
print(result)

# 方式4 in-place
y.add_(x) 
print(y)
```

索引操作：（类似于numpy）

**需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。**

```python
# 取第二列
print(x[:, 1]) 
```

```python
y = x[0,:]
y += 1
print(y)
print(x[0, :]) # 源tensor也被改了了
```

改变大小:如果你想改变一个 tensor 的大小或者形状，你可以使用 torch.view：

```python
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8) # -1是指这一维的维数由其他维度决定
print(x.size(), y.size(), z.size())
```

```
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

注意 **view()** 返回的新**tensor**与源**tensor**共享内存(其实是同一个**tensor**)，也即更改其中的一个，另 外一个也会跟着改变。**(**顾名思义，**view**仅仅是改变了对这个张量的观察⻆度**)**

```python
x += 1
print(x)
print(y) # 也加了了1
```

所以如果我们想返回一个真正新的副本(即不共享内存)该怎么办呢？Pytorch还提供了一 个 reshape() 可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用 clone 创造一个副本然后再使用 view 。

注意：使用 clone 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor 。

如果你有一个元素 tensor ，使用 .item() 来获得这个 value：

```python
x = torch.randn(1) 
print(x) 
print(x.item())
```

PyTorch中的 Tensor 支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考官方文档。

##### 广播机制

当对两个形状不同的 Tensor 按元素运算时，可能会触发广播(broadcasting)机制：先适当复制元素使这两个 Tensor 形状相同后再按元素运算。

```python
x = torch.arange(1, 3).view(1, 2)
print(x)
y = torch.arange(1, 4).view(3, 1)
print(y)
print(x + y)
```

由于 x 和 y 分别是1行2列和3行1列的矩阵，如果要计算 x + y ，那么 x 中第一行的2个元素被广播 (复制)到了第二行和第三行，⽽ y 中第⼀列的3个元素被广播(复制)到了第二列。如此，就可以对2 个3行2列的矩阵按元素相加。
